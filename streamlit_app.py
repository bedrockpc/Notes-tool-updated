import streamlit as st
from utils import inject_custom_css, get_video_id, run_analysis_and_summarize, save_to_pdf
from pathlib import Path
from io import BytesIO 
import json 
from typing import List, Dict, Any, Optional
import re 

# Call the CSS injection function (for base styling)
inject_custom_css()

# --- Configuration and Mapping ---

# Mapping friendly label -> expected JSON key
LABEL_TO_KEY = {
    'Topic Breakdown': 'topic_breakdown',
    'Key Vocabulary': 'key_vocabulary',
    'Formulas & Principles': 'formulas_and_principles',
    'Teacher Insights': 'teacher_insights',
    'Exam Focus Points': 'exam_focus_points',
    'Common Mistakes': 'common_mistakes_explained',
    'Key Points': 'key_points',
    'Short Tricks': 'short_tricks',
    'Must Remembers': 'must_remembers'
}

# Normal Settings Mappings
PAGE_WORD_COUNT_MAP = {
    "3‚Äì4": 800,
    "6‚Äì8": 1500,
    "10‚Äì12": 2200,
    "12+": 3000
}

TIME_MODE_DIVISION_MAP = {
    "Quick": 1,
    "Medium": 3,
    "Detailed": 6
}

# --- Application Setup ---
st.title("üìπ AI-Powered Hyperlinked Video Notes Generator")

# --- Model Context Constants ---
WARNING_THRESHOLD_CHARS = 300000 

# Initialize session state variables
if 'analysis_data' not in st.session_state:
    st.session_state['analysis_data'] = None
if 'api_key_valid' not in st.session_state:
    st.session_state['api_key_valid'] = False
if 'output_filename_base' not in st.session_state:
    st.session_state['output_filename_base'] = "Video_Notes"
if 'chunked_results' not in st.session_state:
    st.session_state['chunked_results'] = []
if 'processing' not in st.session_state:
    st.session_state['processing'] = False

# Initialize new session states for persistence and defaults
if 'num_pages_select' not in st.session_state:
    st.session_state['num_pages_select'] = "12+"
if 'time_mode_select' not in st.session_state:
    st.session_state['time_mode_select'] = "Medium" 
if 'custom_word_count' not in st.session_state:
    st.session_state['custom_word_count'] = 1500
if 'custom_divisions' not in st.session_state:
    st.session_state['custom_divisions'] = 3
if 'settings_mode' not in st.session_state:
    st.session_state['settings_mode'] = "Normal Settings"

# --- üîß CORE HELPER FUNCTIONS ---

def preprocess_transcript(text):
    # (Implementation remains unchanged)
    import re
    pattern = r'\[?(\d{1,2}:\d{2}(?::\d{2})?)\]?' 
    matches = list(re.finditer(pattern, text))
    segments = []
    
    if not matches:
         if text:
             return [{"time": "00:00", "text": text.strip()}]
         return []

    for i in range(len(matches)):
        start = matches[i].end()
        end = matches[i+1].start() if i + 1 < len(matches) else len(text)
        ts = matches[i].group(1)
        segments.append({"time": ts, "text": text[start:end].strip()})
        
    return segments

def split_transcript_by_parts(transcript: str, num_parts: int) -> List[str]:
    """Splits transcript text into equal parts, clamping num_parts to avoid empty strings."""
    text = transcript or ""
    length = len(text)
    
    num_parts = max(1, min(num_parts, length)) 
    part_size = length // num_parts
    
    parts = []
    for i in range(num_parts):
        start = i * part_size
        end = (i + 1) * part_size if i < num_parts - 1 else length
        parts.append(text[start:end])
    return parts

def merge_all_json_outputs(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    # (Implementation remains unchanged)
    LIST_KEYS = list(LABEL_TO_KEY.values())
    
    combined: Dict[str, Any] = {"main_subject": ""}
    
    for key in LIST_KEYS:
        combined[key] = []
        
    for res in results:
        res_normalized = {LABEL_TO_KEY.get(k, k): v for k, v in res.items()}
        
        for k, v in res_normalized.items():
            if k == "main_subject":
                if not combined.get("main_subject") and v:
                    combined["main_subject"] = str(v).strip()
                continue

            if isinstance(v, list) and k in LIST_KEYS:
                combined[k].extend(v)
    
    for k in LIST_KEYS:
        if combined[k]:
            unique_items = []
            seen_hashes = set()
            
            for item in combined[k]:
                item_hash = json.dumps(item, sort_keys=True)
                
                if item_hash not in seen_hashes:
                    unique_items.append(item)
                    seen_hashes.add(item_hash)
            
            combined[k] = unique_items
            
    return combined

# --------------------------------------------------------------------------
# --- Sidebar Setup and Conditional Logic ---
# --------------------------------------------------------------------------

# Set up the two-mode selector
with st.sidebar:
    st.header("üîë Configuration")
    
    # 1. API Key Input (Always visible)
    api_key = st.text_input("Gemini API Key:", type="password")
    if api_key:
        st.session_state['api_key_valid'] = True
        st.success("API Key Entered.")
    else:
        st.session_state['api_key_valid'] = False
        st.warning("Please enter your Gemini API Key.")

    st.markdown("---")
    
    # Mode Selector (Always visible)
    settings_mode = st.radio(
        "‚öôÔ∏è Settings Mode", 
        ["Normal Settings", "Advanced Custom Settings"], 
        key='settings_mode'
    )
    
    st.markdown("---")

    # --- Conditional Settings Panel ---
    
    # Initialize variables that will hold the final configuration values
    final_max_words: int
    final_num_divisions: int
    
    if settings_mode == "Normal Settings":
        st.header("‚ú® Normal Settings")
        
        # 1. Number of Pages (Word Count Control)
        st.subheader("Output Size (Normal)")
        num_pages_choice = st.selectbox(
            "Target PDF Length (Pages):",
            options=list(PAGE_WORD_COUNT_MAP.keys()),
            index=list(PAGE_WORD_COUNT_MAP.keys()).index(st.session_state.get('num_pages_select', "12+")),
            key='num_pages_select', 
            help="Controls the **total length** of the extracted summary."
        )
        # Determine the final max words based on the map
        final_max_words = PAGE_WORD_COUNT_MAP.get(num_pages_choice, 3000)
        st.markdown(f"**Target Word Count:** `{final_max_words}`")

        st.markdown("---")
        
        # 2. Choose Time Mode (Transcript Division Control)
        st.subheader("Processing Speed (Normal)")
        time_mode_choice = st.selectbox(
            "Chunking Mode:",
            options=list(TIME_MODE_DIVISION_MAP.keys()),
            index=list(TIME_MODE_DIVISION_MAP.keys()).index(st.session_state.get('time_mode_select', "Medium")),
            key='time_mode_select', 
            help="Fewer divisions = quicker processing; More divisions = better contextual density."
        )
        # Determine the final divisions based on the map
        final_num_divisions = TIME_MODE_DIVISION_MAP[time_mode_choice]
        st.markdown(f"**Transcript Divisions:** `{final_num_divisions}x`")

    else: # Advanced Custom Settings
        st.header("üî¨ Advanced Custom Settings")
        
        # 1. Custom Word Count (Overrides Pages)
        st.subheader("Output Size (Custom)")
        custom_word_count = st.number_input(
            'Custom Target Word Count:', 
            min_value=500, 
            max_value=10000, 
            value=st.session_state.get('custom_word_count', 1500), 
            step=100, 
            key='custom_word_count', 
            help="Set the precise word limit for the total output summary."
        )
        final_max_words = custom_word_count
        st.markdown(f"**Target Word Count:** `{final_max_words}`")
        
        st.markdown("---")

        # 2. Custom Transcript Divisions (Overrides Time Mode)
        st.subheader("Processing Speed (Custom)")
        custom_divisions = st.slider(
            'Custom Transcript Divisions:',
            min_value=1,
            max_value=10,
            value=st.session_state.get('custom_divisions', 3),
            step=1,
            key='custom_divisions',
            help="The number of parts the transcript will be split into for analysis."
        )
        final_num_divisions = custom_divisions
        st.markdown(f"**Transcript Divisions:** `{final_num_divisions}x`")

    st.markdown("---")
    
    # Model Selection (Always visible, regardless of mode)
    model_choice = st.selectbox(
        "Model Selection:",
        options=["gemini-2.5-pro", "gemini-2.5-flash"],
        index=0, 
        key='model_choice_select', 
        help="Pro = better reasoning, 1M token context. Flash = cheaper, faster, smaller context."
    )

    st.markdown("---")
    
    # 4. YouTube URL Input (Always visible)
    yt_url = st.text_input("YouTube URL (Optional):", help="Provide a URL to enable hyperlinked timestamps in the PDF.")
    video_id = get_video_id(yt_url)
    if video_id:
        st.success(f"Video ID found: {video_id}")
    elif yt_url:
        st.warning("Invalid YouTube URL format. Timestamps will not be hyperlinked.")
    
    st.markdown("---")
    st.header("‚öôÔ∏è Analysis Details")
    
    # B. Checkboxes for Section Selection (Always visible)
    section_options = {
        'Topic Breakdown': True, 'Key Vocabulary': True,
        'Formulas & Principles': True, 'Teacher Insights': False, 
        'Exam Focus Points': True, 'Common Mistakes': False,
        'Key Points': True, 'Short Tricks': False, 'Must Remembers': True      
    }
    
    sections_list = []
    st.subheader("Select Output Sections")
    for label, default_val in section_options.items():
        if st.checkbox(label, value=default_val):
            sections_list.append(label)

    st.markdown("---")
    
    # G. Custom Filename Input (Always visible)
    if video_id:
        st.session_state['output_filename_base'] = f"Notes_{video_id}"
    
    output_filename_base = st.session_state['output_filename_base']
    output_filename = st.text_input(
        "Base Name for PDF file:",
        value=output_filename_base + ".pdf",
        key="output_filename_input"
    )
    
# --------------------------------------------------------------------------
# --- Main Content: Transcript Input, Button, and Output ---
# --------------------------------------------------------------------------

# üí° Dual Format Selection Radio Button
format_choice = st.radio(
    "Choose Reading Format:",
    options=["Default (Compact)", "Easier Read (Spacious & Highlighted)"],
    index=0,
    horizontal=True,
    key='pdf_format_choice',
    help="Easier Read format adds vertical spacing between lines and enables content highlighting."
)
st.markdown("---")

st.subheader("Transcript Input")
transcript_text = st.text_area(
    'Paste the video transcript here (must include timestamps for best results):',
    height=300,
    placeholder="[00:00] Welcome to the lesson. [00:45] We start with Topic A..."
)

# Optional Transcript Warning
if len(transcript_text) > WARNING_THRESHOLD_CHARS and model_choice == "gemini-2.5-flash":
    st.warning(f"‚ö†Ô∏è **Long Transcript Detected!** The text is over {WARNING_THRESHOLD_CHARS} characters. We recommend selecting **Gemini 2.5 Pro** or increasing the divisions to avoid context overflow with Flash.")

user_prompt_input = st.text_area(
    'Refine AI Focus (Optional Prompt):',
    value="Ensure the output is highly condensed and only focus on practical applications and examples.",
    height=100
)

# E. The Analysis Trigger Button
can_run = transcript_text and st.session_state['api_key_valid']
run_analysis = st.button(
    f"üöÄ Generate Notes using {model_choice}", 
    type="primary", 
    disabled=not can_run or st.session_state['processing']
) 

is_easy_read = format_choice.startswith("Easier Read")

if run_analysis and not st.session_state['processing']:
    
    # Print the active configuration for debugging
    print(f"\n--- DEBUG RUN START ---")
    print(f"| Settings Mode: {settings_mode}")
    print(f"| Final Max Words: {final_max_words}")
    print(f"| Final Divisions: {final_num_divisions}")
    print(f"| Model: {model_choice}")
    print(f"| Video ID: {video_id}")
    print(f"--- DEBUG RUN END ---")
    
    st.session_state['processing'] = True
    st.session_state['chunked_results'] = []
    
    try:
        # Determine the number of parts to use (1 for Pro, or the configured division count)
        num_parts_to_use = 1 
        if model_choice == "gemini-2.5-flash":
            num_parts_to_use = final_num_divisions
        
        transcript_parts = split_transcript_by_parts(transcript_text, num_parts_to_use)
        
        st.info(f"Analyzing in **{len(transcript_parts)}** sequential part(s) using **{model_choice}** (Divisions: {num_parts_to_use}).")

        # Chunked Execution
        status_bar = st.progress(0, text="Starting analysis...")
        
        sections_list_keys = [LABEL_TO_KEY.get(lbl, lbl) for lbl in sections_list]

        for i, part in enumerate(transcript_parts, start=1):
            status_bar.progress(
                i / len(transcript_parts), 
                text=f'Analyzing Part {i} of {len(transcript_parts)}... (Model: {model_choice})'
            )
            
            preprocessed_part = preprocess_transcript(part)

            # Pass the currently configured max_words (final_max_words)
            data_json, error_msg, full_prompt = run_analysis_and_summarize(
                api_key, preprocessed_part, final_max_words, sections_list_keys, user_prompt_input, model_choice, is_easy_read
            )
            
            if data_json:
                st.session_state['chunked_results'].append(data_json)
            else:
                st.error(f"Analysis failed for Part {i}. Error: {error_msg}")
                st.session_state['chunked_results'] = []
                break

        status_bar.empty()

        if st.session_state['chunked_results']:
            st.success(f"Analysis complete for all {len(st.session_state['chunked_results'])} parts.")
            st.session_state['pdf_ready'] = True
        else:
            st.session_state['pdf_ready'] = False
            
    finally:
        st.session_state['processing'] = False

st.markdown("---")

# 7. Output Options and Download Section
if st.session_state['chunked_results']:
    st.subheader("üß© Output Options")
    combine_choice = st.radio(
        "Choose how to handle analyzed chunks:",
        options=["üîó Combine all outputs into one file", "üì¶ Download each part separately"],
        index=0,
        horizontal=True,
        key='combine_choice_radio',
        help="You can merge all analyzed chunks into a single hyperlinked PDF, or keep each chunk's output separate."
    )

    current_dir = Path(__file__).parent

    if combine_choice.startswith("üîó"):
        st.subheader("Single Merged PDF")
        
        combined_data = merge_all_json_outputs(st.session_state['chunked_results'])
        
        pdf_output = BytesIO()
        try:
            with st.spinner("Generating combined PDF..."):
                save_to_pdf(combined_data, video_id, current_dir, pdf_output, format_choice)
            
            st.download_button(
                label=f"‚¨áÔ∏è Download Merged Notes: {output_filename_base}.pdf",
                data=pdf_output,
                file_name=output_filename, 
                mime="application/pdf" 
            )
        except Exception as e:
            st.error(f"Error generating merged PDF: {e}")
            st.warning("Ensure font files (NotoSans-*.ttf) are in the main directory.")

    else:
        st.subheader("Separate PDF Downloads")
        st.info("Each part represents a section of the original transcript.")

        for i, part_data in enumerate(st.session_state['chunked_results'], start=1):
            pdf_output = BytesIO()
            
            try:
                with st.spinner(f"Preparing Part {i}..."):
                    save_to_pdf(part_data, video_id, current_dir, pdf_output, format_choice)
                
                st.download_button(
                    label=f"‚¨áÔ∏è Download Part {i}",
                    data=pdf_output,
                    file_name=f"{output_filename_base}_part{i}.pdf",
                    mime="application/pdf",
                    key=f'download_part_{i}'
                )
            except Exception as e:
                st.error(f"Error generating Part {i} PDF: {e}")
                break

st.markdown("---")

if not st.session_state['chunked_results'] and st.session_state.get('pdf_ready'):
    st.warning("Analysis ran successfully, but no output was generated. Please verify your transcript and settings.")